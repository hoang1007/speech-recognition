{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from typing import Tuple, Iterable, Optional\n",
    "import re\n",
    "import torch\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define The Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "FINETUNE_LR = 1e-5\n",
    "LR = 1e-4\n",
    "EPOCHS = 30\n",
    "TOTAL_STEPS = (42_000 // BATCH_SIZE) * EPOCHS\n",
    "WARMUP_STEPS = int(TOTAL_STEPS * 0.1)\n",
    "CONSTANT_STEPS = int(TOTAL_STEPS * 0.4)\n",
    "WEIGHT_DECAY = 5e-3\n",
    "\n",
    "CKPT_DIR = \"ckpt\"\n",
    "LOG_DIR = \"logs\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert original dataset to WebDataset tar file for faster loading on Google Colab\n",
    "from src.datamodule import VLSP2020Dataset, VLSP2020TarDataset\n",
    "\n",
    "# from torch.utils.data import random_split\n",
    "\n",
    "# dts = VLSP2020Dataset(\"../data/vlsp2020_train_set_02\")\n",
    "# train_set, val_set = random_split(dts, [42_000, 14_427])\n",
    "\n",
    "# VLSP2020TarDataset(\"../data/vlsp2020_train_set.tar\").convert(train_set)\n",
    "# VLSP2020TarDataset(\"../data/vlsp2020_val_set.tar\").convert(val_set)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get dataloader and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = VLSP2020TarDataset(\"../data/vlsp2020_train_set.tar\").load()\n",
    "val_dataset = VLSP2020TarDataset(\"../data/vlsp2020_val_set.tar\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "from src.datamodule.vlsp2020 import get_dataloader\n",
    "\n",
    "\n",
    "def remove_punctuation(text: str):\n",
    "    return text.translate(str.maketrans(\"\", \"\", string.punctuation)).lower()\n",
    "\n",
    "\n",
    "train_loader = get_dataloader(\n",
    "    train_dataset,\n",
    "    return_transcript=True,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    target_transform=remove_punctuation,\n",
    ")\n",
    "\n",
    "val_loader = get_dataloader(\n",
    "    val_dataset,\n",
    "    return_transcript=True,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    target_transform=remove_punctuation,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('và hình như tôi cảm giác là qua từng vòng thi thì cái họng của bạn tốt hơn rồi thì phải', 'đến đây thì mọi bắt đầu thấy được cuộc chiến khốc liệt như thế nào rồi đúng không ạ họ rất là tính tay rất kĩ thưa quý vị chọn ô màu của mình mời em đọc câu tiếp theo'), (tensor([-0.0012,  0.0088,  0.0054,  ..., -0.0038, -0.0009,  0.0046]), tensor([-0.0760, -0.0811, -0.0728,  ..., -0.0032, -0.0034, -0.0035])))\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein_distance(source: Tuple[str], target: Tuple[str]):\n",
    "    \"\"\"\n",
    "    Compute the Levenshtein distance between two sequences.\n",
    "    \"\"\"\n",
    "\n",
    "    n, m = len(source), len(target)\n",
    "    if n > m:\n",
    "        # Make sure n <= m, to use O(min(n,m)) space\n",
    "        source, target = target, source\n",
    "        n, m = m, n\n",
    "\n",
    "    current_row = range(n + 1)  # Keep current and previous row, not entire matrix\n",
    "    for i in range(1, m + 1):\n",
    "        previous_row, current_row = current_row, [i] + [0] * n\n",
    "        for j in range(1, n + 1):\n",
    "            add, delete, change = (\n",
    "                previous_row[j] + 1,\n",
    "                current_row[j - 1] + 1,\n",
    "                previous_row[j - 1],\n",
    "            )\n",
    "            if source[j - 1] != target[i - 1]:\n",
    "                change += 1\n",
    "            current_row[j] = min(add, delete, change)\n",
    "\n",
    "    distance = current_row[n]\n",
    "\n",
    "    del current_row\n",
    "    del previous_row\n",
    "\n",
    "    return distance\n",
    "\n",
    "\n",
    "def word_error_rate(prediction: str, transcript: str):\n",
    "    pattern = r\"\\W+\"\n",
    "\n",
    "    prediction = re.split(pattern, prediction)\n",
    "    transcript = re.split(pattern, transcript)\n",
    "\n",
    "    return levenshtein_distance(prediction, transcript) / len(transcript)\n",
    "\n",
    "\n",
    "def character_error_rate(prediction: str, transcript: str):\n",
    "    return levenshtein_distance(prediction, transcript) / len(transcript)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class VocabBuilder:\n",
    "    DELIM_TOKEN = \"|\"\n",
    "    UNK_TOKEN = \"<unk>\"\n",
    "    PAD_TOKEN = \"<pad>\"\n",
    "\n",
    "    def __init__(self, vocab_file: str):\n",
    "        self.vocab_file = vocab_file\n",
    "        self._vocab_set = set()\n",
    "\n",
    "    def add(self, texts: Iterable[str]):\n",
    "        for t in texts:\n",
    "            self._vocab_set.update(t)\n",
    "\n",
    "    def build(self):\n",
    "        self._vocab_dict = {c: i for i, c in enumerate(self._vocab_set)}\n",
    "\n",
    "        # replace space with pipe for clearer visualization\n",
    "        self._vocab_dict[self.DELIM_TOKEN] = self._vocab_dict[\" \"]\n",
    "        del self._vocab_dict[\" \"]\n",
    "\n",
    "        # add unknown token so that model can handle unseen characters\n",
    "        self._vocab_dict[self.UNK_TOKEN] = len(self._vocab_dict)\n",
    "\n",
    "        # add padding token for CTC\n",
    "        self._vocab_dict[self.PAD_TOKEN] = len(self._vocab_dict)\n",
    "\n",
    "        return self._vocab_dict\n",
    "\n",
    "    @property\n",
    "    def vocab_dict(self):\n",
    "        return self._vocab_dict\n",
    "\n",
    "    def save(self):\n",
    "        with open(self.vocab_file, \"w\") as f:\n",
    "            json.dump(self.vocab_dict, f, ensure_ascii=False)\n",
    "\n",
    "    def load(self):\n",
    "        with open(self.vocab_file, \"r\") as f:\n",
    "            self._vocab_dict = json.load(f)\n",
    "\n",
    "        return self._vocab_dict\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech Recognizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import LightningModule\n",
    "from transformers import (\n",
    "    Wav2Vec2ForPreTraining,\n",
    "    Wav2Vec2CTCTokenizer,\n",
    "    Wav2Vec2FeatureExtractor,\n",
    ")\n",
    "from torchmetrics import MeanMetric\n",
    "\n",
    "from src.utils.scheduler import TriStateScheduler\n",
    "\n",
    "class SpeechRecognizer(LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        wav2vec2: Wav2Vec2ForPreTraining,\n",
    "        tokenizer: Wav2Vec2CTCTokenizer,\n",
    "        feature_extractor: Wav2Vec2FeatureExtractor,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        self.hidden_size = wav2vec2.config.hidden_size\n",
    "        self.vocab_size = tokenizer.vocab_size\n",
    "\n",
    "        self.wav2vec2 = wav2vec2\n",
    "        self.wav2vec2.freeze_feature_extractor()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.feature_extractor = feature_extractor\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(0.1)\n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.hidden_size, self.hidden_size // 2),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Linear(self.hidden_size // 2, self.vocab_size),\n",
    "        )\n",
    "\n",
    "        self.criterion = torch.nn.CTCLoss(blank=tokenizer.pad_token_id)\n",
    "\n",
    "        self.train_loss = MeanMetric()\n",
    "\n",
    "    def forward(self, waveforms: Tuple[torch.Tensor], transcripts: Tuple[str] = None):\n",
    "        # convert torch.Tensor to numpy.ndarray\n",
    "        waveforms = tuple(waveform.cpu().numpy() for waveform in waveforms)\n",
    "\n",
    "        extracted = self.feature_extractor(\n",
    "            waveforms,\n",
    "            sampling_rate=16000,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "\n",
    "        outputs = self.wav2vec2(\n",
    "            extracted.input_values,\n",
    "            attention_mask=extracted.attention_mask,\n",
    "        )\n",
    "\n",
    "        # hidden_states.shape == (batch_size, sequence_length, hidden_size)\n",
    "        hidden_states = outputs[0]\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "\n",
    "        # logits.shape == (batch_size, sequence_length, vocab_size)\n",
    "        logits = self.fc(hidden_states)\n",
    "\n",
    "        if transcripts is not None:\n",
    "            # get the length of valids sequence\n",
    "            input_lengths = self.wav2vec2._get_feat_extract_output_lengths(\n",
    "                extracted.attention_mask.sum(-1)\n",
    "            ).to(torch.long)\n",
    "\n",
    "            # tokenize transcripts\n",
    "            target_ids, target_lengths = self.tokenizer(\n",
    "                transcripts, padding=True, return_length=True, return_tensors=\"pt\"\n",
    "            ).values()\n",
    "\n",
    "            # (batch_size, sequence_length, vocab_size) -> (sequence_length, batch_size, vocab_size)\n",
    "            log_probs = torch.nn.functional.log_softmax(logits, dim=-1).transpose_(0, 1)\n",
    "\n",
    "            # compute loss\n",
    "            loss = self.criterion(log_probs, target_ids, input_lengths, target_lengths)\n",
    "\n",
    "            return loss, logits\n",
    "        else:\n",
    "            return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        transcripts, waveforms = batch\n",
    "\n",
    "        loss, logits = self(waveforms, transcripts)\n",
    "\n",
    "        self.train_loss(loss)\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            self.log(\"train/loss\", self.train_loss, on_step=True, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        transcripts, waveforms = batch\n",
    "\n",
    "        logits = self(waveforms)\n",
    "\n",
    "        predicted_ids = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "\n",
    "        predicted_texts = self.tokenizer.batch_decode(predicted_ids)\n",
    "\n",
    "        wer = word_error_rate(predicted_texts, transcripts)\n",
    "        cer = character_error_rate(predicted_texts, transcripts)\n",
    "\n",
    "        self.log(\"val/wer\", wer, on_epoch=True)\n",
    "        self.log(\"val/cer\", cer, on_epoch=True)\n",
    "\n",
    "        return wer, cer\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        optimizer = torch.optim.AdamW([\n",
    "            {\"params\": self.wav2vec2.parameters(), \"lr\": FINETUNE_LR},\n",
    "            {\"params\": self.fc.parameters(), \"lr\": LR},\n",
    "        ], lr=LR, weight_decay=WEIGHT_DECAY\n",
    "        )\n",
    "\n",
    "        scheduler = TriStateScheduler(\n",
    "            optimizer,\n",
    "            total_steps=TOTAL_STEPS,\n",
    "            warmup_steps=WARMUP_STEPS,\n",
    "            constant_steps=CONSTANT_STEPS,\n",
    "            factor=1e-3\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"interval\": \"step\",\n",
    "                \"frequency\": 1,\n",
    "            }\n",
    "        }\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at nguyenvulebinh/wav2vec2-base-vietnamese-250h were not used when initializing Wav2Vec2ForPreTraining: ['lm_head.bias', 'lm_head.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2ForPreTraining from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForPreTraining from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForPreTraining were not initialized from the model checkpoint at nguyenvulebinh/wav2vec2-base-vietnamese-250h and are newly initialized: ['quantizer.weight_proj.weight', 'project_q.bias', 'quantizer.weight_proj.bias', 'project_hid.bias', 'quantizer.codevectors', 'project_q.weight', 'project_hid.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"nguyenvulebinh/wav2vec2-base-vietnamese-250h\"\n",
    "\n",
    "wav2vec2 = Wav2Vec2ForPreTraining.from_pretrained(model_name)\n",
    "tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(model_name)\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SpeechRecognizer(wav2vec2, tokenizer, feature_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "trainer = Trainer(\n",
    "    accelerator=\"cpu\",\n",
    "    callbacks=[\n",
    "        ModelCheckpoint(CKPT_DIR, monitor=\"val/wer\", mode=\"min\", save_top_k=1)\n",
    "    ],\n",
    "    logger=TensorBoardLogger(LOG_DIR),\n",
    "    max_epochs=EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing logger folder: logs/lightning_logs\n",
      "\n",
      "  | Name       | Type                   | Params\n",
      "------------------------------------------------------\n",
      "0 | wav2vec2   | Wav2Vec2ForPreTraining | 95.0 M\n",
      "1 | dropout    | Dropout                | 0     \n",
      "2 | fc         | Sequential             | 337 K \n",
      "3 | criterion  | CTCLoss                | 0     \n",
      "4 | train_loss | MeanMetric             | 0     \n",
      "------------------------------------------------------\n",
      "91.2 M    Trainable params\n",
      "4.2 M     Non-trainable params\n",
      "95.4 M    Total params\n",
      "381.529   Total estimated model params size (MB)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model, train_loader, val_loader)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:582\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`Trainer.fit()` requires a `LightningModule`, got: \u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    581\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[0;32m--> 582\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    583\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    584\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:38\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     37\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     40\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     41\u001b[0m     trainer\u001b[39m.\u001b[39m_call_teardown_hook()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:624\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    617\u001b[0m ckpt_path \u001b[39m=\u001b[39m ckpt_path \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresume_from_checkpoint\n\u001b[1;32m    618\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_set_ckpt_path(\n\u001b[1;32m    619\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[1;32m    620\u001b[0m     ckpt_path,  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    621\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    622\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    623\u001b[0m )\n\u001b[0;32m--> 624\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mckpt_path)\n\u001b[1;32m    626\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    627\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1061\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1057\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mrestore_training_state()\n\u001b[1;32m   1059\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[0;32m-> 1061\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m   1063\u001b[0m log\u001b[39m.\u001b[39mdetail(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1064\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_teardown()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1140\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1138\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[1;32m   1139\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n\u001b[0;32m-> 1140\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_train()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1152\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1149\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_train\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1150\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pre_training_routine()\n\u001b[0;32m-> 1152\u001b[0m     \u001b[39mwith\u001b[39;00m isolate_rng():\n\u001b[1;32m   1153\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_sanity_check()\n\u001b[1;32m   1155\u001b[0m     \u001b[39m# enable train mode\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/contextlib.py:135\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkwds, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc\n\u001b[1;32m    134\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 135\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgen)\n\u001b[1;32m    136\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mgenerator didn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt yield\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_lightning/utilities/seed.py:42\u001b[0m, in \u001b[0;36misolate_rng\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39m@contextmanager\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39misolate_rng\u001b[39m() \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Generator[\u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m]:\n\u001b[1;32m     29\u001b[0m     \u001b[39m\"\"\"A context manager that resets the global random state on exit to what it was before entering.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \n\u001b[1;32m     31\u001b[0m \u001b[39m    It supports isolating the states for PyTorch, Numpy, and Python built-in random number generators.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39m        tensor([0.7576])\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     states \u001b[39m=\u001b[39m _collect_rng_states()\n\u001b[1;32m     43\u001b[0m     \u001b[39myield\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     _set_rng_states(states)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/lightning_lite/utilities/seed.py:112\u001b[0m, in \u001b[0;36m_collect_rng_states\u001b[0;34m()\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_collect_rng_states\u001b[39m() \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, Any]:\n\u001b[1;32m    109\u001b[0m     \u001b[39m\"\"\"Collect the global random state of :mod:`torch`, :mod:`torch.cuda`, :mod:`numpy` and Python.\"\"\"\u001b[39;00m\n\u001b[1;32m    110\u001b[0m     \u001b[39mreturn\u001b[39;00m {\n\u001b[1;32m    111\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtorch\u001b[39m\u001b[39m\"\u001b[39m: torch\u001b[39m.\u001b[39mget_rng_state(),\n\u001b[0;32m--> 112\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtorch.cuda\u001b[39m\u001b[39m\"\u001b[39m: torch\u001b[39m.\u001b[39;49mcuda\u001b[39m.\u001b[39;49mget_rng_state_all(),\n\u001b[1;32m    113\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mnumpy\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mget_state(),\n\u001b[1;32m    114\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpython\u001b[39m\u001b[39m\"\u001b[39m: python_get_rng_state(),\n\u001b[1;32m    115\u001b[0m     }\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/cuda/random.py:39\u001b[0m, in \u001b[0;36mget_rng_state_all\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m results \u001b[39m=\u001b[39m []\n\u001b[1;32m     38\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(device_count()):\n\u001b[0;32m---> 39\u001b[0m     results\u001b[39m.\u001b[39mappend(get_rng_state(i))\n\u001b[1;32m     40\u001b[0m \u001b[39mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/cuda/random.py:22\u001b[0m, in \u001b[0;36mget_rng_state\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_rng_state\u001b[39m(device: Union[\u001b[39mint\u001b[39m, \u001b[39mstr\u001b[39m, torch\u001b[39m.\u001b[39mdevice] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m     13\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Returns the random number generator state of the specified GPU as a ByteTensor.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39m        This function eagerly initializes CUDA.\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m     _lazy_init()\n\u001b[1;32m     23\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(device, \u001b[39mstr\u001b[39m):\n\u001b[1;32m     24\u001b[0m         device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:229\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mCUDA_MODULE_LOADING\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39menviron:\n\u001b[1;32m    228\u001b[0m     os\u001b[39m.\u001b[39menviron[\u001b[39m'\u001b[39m\u001b[39mCUDA_MODULE_LOADING\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mLAZY\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 229\u001b[0m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_cuda_init()\n\u001b[1;32m    230\u001b[0m \u001b[39m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[39m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[39m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    233\u001b[0m _tls\u001b[39m.\u001b[39mis_initializing \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero."
     ]
    }
   ],
   "source": [
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([\"xin chào 😃\", \"cô gái nông thôn\"], return_tensors=\"pt\", padding=True, return_length=True).input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 81,  57,  26,  46,  49,  71,  24, 105,  46, 108, 109, 109, 109, 109,\n",
       "         109, 109],\n",
       "        [ 49,  30,  46,  58,  62,  57,  46,  26,  30,  26,  58,  46,  56,  71,\n",
       "          30,  26]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xin chào <unk>', 'cô gái nông thôn']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ẻ': 0,\n",
       " '6': 1,\n",
       " 'ụ': 2,\n",
       " 'í': 3,\n",
       " '3': 4,\n",
       " 'ỹ': 5,\n",
       " 'ý': 6,\n",
       " 'ẩ': 7,\n",
       " 'ở': 8,\n",
       " 'ề': 9,\n",
       " 'õ': 10,\n",
       " '7': 11,\n",
       " 'ê': 12,\n",
       " 'ứ': 13,\n",
       " 'ỏ': 14,\n",
       " 'v': 15,\n",
       " 'ỷ': 16,\n",
       " 'a': 17,\n",
       " 'l': 18,\n",
       " 'ự': 19,\n",
       " 'q': 20,\n",
       " 'ờ': 21,\n",
       " 'j': 22,\n",
       " 'ố': 23,\n",
       " 'à': 24,\n",
       " 'ỗ': 25,\n",
       " 'n': 26,\n",
       " 'é': 27,\n",
       " 'ủ': 28,\n",
       " 'у': 29,\n",
       " 'ô': 30,\n",
       " 'u': 31,\n",
       " 'y': 32,\n",
       " 'ằ': 33,\n",
       " '4': 34,\n",
       " 'w': 35,\n",
       " 'b': 36,\n",
       " 'ệ': 37,\n",
       " 'ễ': 38,\n",
       " 's': 39,\n",
       " 'ì': 40,\n",
       " 'ầ': 41,\n",
       " 'ỵ': 42,\n",
       " '8': 43,\n",
       " 'd': 44,\n",
       " 'ể': 45,\n",
       " 'r': 47,\n",
       " 'ũ': 48,\n",
       " 'c': 49,\n",
       " 'ạ': 50,\n",
       " '9': 51,\n",
       " 'ế': 52,\n",
       " 'ù': 53,\n",
       " 'ỡ': 54,\n",
       " '2': 55,\n",
       " 't': 56,\n",
       " 'i': 57,\n",
       " 'g': 58,\n",
       " '́': 59,\n",
       " 'ử': 60,\n",
       " '̀': 61,\n",
       " 'á': 62,\n",
       " '0': 63,\n",
       " 'ậ': 64,\n",
       " 'e': 65,\n",
       " 'ộ': 66,\n",
       " 'm': 67,\n",
       " 'ẳ': 68,\n",
       " 'ợ': 69,\n",
       " 'ĩ': 70,\n",
       " 'h': 71,\n",
       " 'â': 72,\n",
       " 'ú': 73,\n",
       " 'ọ': 74,\n",
       " 'ồ': 75,\n",
       " 'ặ': 76,\n",
       " 'f': 77,\n",
       " 'ữ': 78,\n",
       " 'ắ': 79,\n",
       " 'ỳ': 80,\n",
       " 'x': 81,\n",
       " 'ó': 82,\n",
       " 'ã': 83,\n",
       " 'ổ': 84,\n",
       " 'ị': 85,\n",
       " '̣': 86,\n",
       " 'z': 87,\n",
       " 'ả': 88,\n",
       " 'đ': 89,\n",
       " 'è': 90,\n",
       " 'ừ': 91,\n",
       " 'ò': 92,\n",
       " 'ẵ': 93,\n",
       " '1': 94,\n",
       " 'ơ': 95,\n",
       " 'k': 96,\n",
       " 'ẫ': 97,\n",
       " 'p': 98,\n",
       " 'ấ': 99,\n",
       " 'ẽ': 100,\n",
       " 'ỉ': 101,\n",
       " 'ớ': 102,\n",
       " 'ẹ': 103,\n",
       " 'ă': 104,\n",
       " 'o': 105,\n",
       " 'ư': 106,\n",
       " '5': 107,\n",
       " '|': 46,\n",
       " '<unk>': 108,\n",
       " '<pad>': 109,\n",
       " '<s>': 110,\n",
       " '</s>': 111}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.modules import Wav2Vec2Processor\n",
    "from src.model import Wav2Vec2PretrainingModule\n",
    "import torch\n",
    "\n",
    "\n",
    "def speech_to_text(waveforms: Tuple[torch.Tensor, ...]):\n",
    "\n",
    "    batched_waveforms, wavelengths = Wav2Vec2Processor()(waveforms)\n",
    "    attention_masks = Wav2Vec2PretrainingModule._compute_attention_mask(wavelengths)\n",
    "\n",
    "    logits = model(batched_waveforms, attention_mask=attention_masks).logits\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    return processor.batch_decode(predicted_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character error rate: 25.47%\n",
      "Word error rate: 32.33%\n"
     ]
    }
   ],
   "source": [
    "cer, wer = 0, 0\n",
    "n_items = 0\n",
    "\n",
    "for batch_idx, batch in enumerate(dataloader):\n",
    "    transcripts, waveforms = batch\n",
    "    predicted_transcripts = speech_to_text(waveforms)\n",
    "\n",
    "    for predicted_transcript, transcript in zip(predicted_transcripts, transcripts):\n",
    "        cer += character_error_rate(predicted_transcript, transcript)\n",
    "        wer += word_error_rate(predicted_transcript, transcript)\n",
    "\n",
    "        n_items += 1\n",
    "\n",
    "    if batch_idx > 100:\n",
    "        break\n",
    "\n",
    "    del transcript\n",
    "    del waveforms\n",
    "    del predicted_transcript\n",
    "    del batch\n",
    "\n",
    "cer /= n_items\n",
    "wer /= n_items\n",
    "\n",
    "print(f\"Character error rate: {cer:.2%}\")\n",
    "print(f\"Word error rate: {wer:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "      <th>predicted_transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trước khi bắt đầu khởi hành hiển dự định đạp xe vào bốn giờ ba mươi phút sáng hàng ngày</td>\n",
       "      <td>trước khi bắt đầu khởi hành hiện dự định đạp xe và bốn giờ ba mươi phút sáng hằng ngày</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;unk&gt; ngon</td>\n",
       "      <td>ớnđyvn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xin được cảm ơn những chia sẻ đầy cảm xúc của các khách mời đã giúp cho chúng tôi những người thuộc nhiều thế hệ được lớn lên trong hòa bình thêm trân trọng những mất mát hi sinh của thế hệ đi trước để có được một việt nam hòa bình giúp cho chúng tôi</td>\n",
       "      <td>xin được cảm ơn những chia sẻ đầy cảm xúc của các khách mời đã giúp cho chúng tôi những người thuộc nhiều thế hệ được lớn lên trong hòa bình thêm trân trọng những mất mát hi sinh của thấy đi trước để có được một việt nam hòa bình giúp cho chúng tôi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nguyễn trang đem thi thể chúa trịnh nộp cho quân tây sơn</td>\n",
       "      <td>nguyễn trang đem thi thể chúa trịnh nộp cho quân tây sơn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>theo đó chủ mưu trong vụ án là bị cáo nguyễn minh hùng bị tuyên mười bảy năm tù với vai trò chỉ</td>\n",
       "      <td>theo đó chủ mưu trong vụ án là bị cáo nguyễn minh hùng bị tuyên mười bảy năm tù với vai trò chỉ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bây giờ sẽ là phần thi cuối cùng và cũng là &lt;unk&gt;</td>\n",
       "      <td>bây giờ sẽ là phần thi cuối cùng và cũn là</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>đồng thời lại nghe theo kiến nghị của quân sư ông cho đúc trở lại tiền ngũ thù khôi phục lưu thông tiền tệ trong địa bàn đem đến tiện lợi cho sinh hoạt của nhân dân không còn phải dùng xếp vải làm phương tiện thanh toán</td>\n",
       "      <td>đồng thời lại nghe theo kýn nghị của quân sư ông cho đúc trở lại tiền ngủ thù khôi phục lưu thông tiền tệ trong địa bàn đem đến tiền lợi cho sinh hoạt của nhân dân không còn phải dùng xếp vãi làm phương tiện thanh toán</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>chính quyền của tổng thống trăm nhằm cắt giảm các chương trình trợ cấp an sinh xã hội</td>\n",
       "      <td>chính quyền của tổng thống trămp nhắm cắt giảm các chương trình trơa cấp an sinh xã hộicc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>văn học việt nam là sự tích hợp từ hai dòng văn học dân gian và văn học viết của những người dùng tiếng việt</td>\n",
       "      <td>văn học việt nam là sự tích hợp từ hai dòng văn học dân gian và văn học viết của những người dùng tiếng việt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ui tình</td>\n",
       "      <td>ừừừ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>dân trí sẽ tiếp tục cập nhật diễn biến của buổi họp báo này</td>\n",
       "      <td>dân chí sẽ tiếp tục cập nhật diễn biến của buổi họp báo nàys</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>và đặc biệt là các bạn sẽ không thích các cuộc tán gẫu vặt tại vì bạn cảm thấy mệt mỏi khi phải duy trì một cuộc nói chuyện thực ra người hướng nội cũng có nhiều tuýp hướng nội khác nhau nhá</td>\n",
       "      <td>nhóm và đặc biệt là các bạn sại không thích những cuộc tán gỗ vặt tại vì bạn cảm thấy mệt mỏi khi phải duy trì một  cuộc nói chuyện thực ra người hướng nội cũng có nhiều cái tuýp hướng nội khác nhau</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                    transcript  \\\n",
       "0                                                                                                                                                                      trước khi bắt đầu khởi hành hiển dự định đạp xe vào bốn giờ ba mươi phút sáng hàng ngày   \n",
       "1                                                                                                                                                                                                                                                   <unk> ngon   \n",
       "2   xin được cảm ơn những chia sẻ đầy cảm xúc của các khách mời đã giúp cho chúng tôi những người thuộc nhiều thế hệ được lớn lên trong hòa bình thêm trân trọng những mất mát hi sinh của thế hệ đi trước để có được một việt nam hòa bình giúp cho chúng tôi   \n",
       "3                                                                                                                                                                                                     nguyễn trang đem thi thể chúa trịnh nộp cho quân tây sơn   \n",
       "4                                                                                                                                                              theo đó chủ mưu trong vụ án là bị cáo nguyễn minh hùng bị tuyên mười bảy năm tù với vai trò chỉ   \n",
       "5                                                                                                                                                                                                            bây giờ sẽ là phần thi cuối cùng và cũng là <unk>   \n",
       "6                                  đồng thời lại nghe theo kiến nghị của quân sư ông cho đúc trở lại tiền ngũ thù khôi phục lưu thông tiền tệ trong địa bàn đem đến tiện lợi cho sinh hoạt của nhân dân không còn phải dùng xếp vải làm phương tiện thanh toán   \n",
       "7                                                                                                                                                                        chính quyền của tổng thống trăm nhằm cắt giảm các chương trình trợ cấp an sinh xã hội   \n",
       "8                                                                                                                                                 văn học việt nam là sự tích hợp từ hai dòng văn học dân gian và văn học viết của những người dùng tiếng việt   \n",
       "9                                                                                                                                                                                                                                                      ui tình   \n",
       "10                                                                                                                                                                                                 dân trí sẽ tiếp tục cập nhật diễn biến của buổi họp báo này   \n",
       "11                                                              và đặc biệt là các bạn sẽ không thích các cuộc tán gẫu vặt tại vì bạn cảm thấy mệt mỏi khi phải duy trì một cuộc nói chuyện thực ra người hướng nội cũng có nhiều tuýp hướng nội khác nhau nhá   \n",
       "\n",
       "                                                                                                                                                                                                                                        predicted_transcript  \n",
       "0                                                                                                                                                                     trước khi bắt đầu khởi hành hiện dự định đạp xe và bốn giờ ba mươi phút sáng hằng ngày  \n",
       "1                                                                                                                                                                                                                                                     ớnđyvn  \n",
       "2   xin được cảm ơn những chia sẻ đầy cảm xúc của các khách mời đã giúp cho chúng tôi những người thuộc nhiều thế hệ được lớn lên trong hòa bình thêm trân trọng những mất mát hi sinh của thấy đi trước để có được một việt nam hòa bình giúp cho chúng tôi  \n",
       "3                                                                                                                                                                                                   nguyễn trang đem thi thể chúa trịnh nộp cho quân tây sơn  \n",
       "4                                                                                                                                                            theo đó chủ mưu trong vụ án là bị cáo nguyễn minh hùng bị tuyên mười bảy năm tù với vai trò chỉ  \n",
       "5                                                                                                                                                                                                                 bây giờ sẽ là phần thi cuối cùng và cũn là  \n",
       "6                                 đồng thời lại nghe theo kýn nghị của quân sư ông cho đúc trở lại tiền ngủ thù khôi phục lưu thông tiền tệ trong địa bàn đem đến tiền lợi cho sinh hoạt của nhân dân không còn phải dùng xếp vãi làm phương tiện thanh toán  \n",
       "7                                                                                                                                                                  chính quyền của tổng thống trămp nhắm cắt giảm các chương trình trơa cấp an sinh xã hộicc  \n",
       "8                                                                                                                                               văn học việt nam là sự tích hợp từ hai dòng văn học dân gian và văn học viết của những người dùng tiếng việt  \n",
       "9                                                                                                                                                                                                                                                        ừừừ  \n",
       "10                                                                                                                                                                                              dân chí sẽ tiếp tục cập nhật diễn biến của buổi họp báo nàys  \n",
       "11                                                    nhóm và đặc biệt là các bạn sại không thích những cuộc tán gỗ vặt tại vì bạn cảm thấy mệt mỏi khi phải duy trì một  cuộc nói chuyện thực ra người hướng nội cũng có nhiều cái tuýp hướng nội khác nhau  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "m = []\n",
    "\n",
    "for batch_idx, batch in enumerate(dataloader):\n",
    "    if batch_idx > 5:\n",
    "        break\n",
    "\n",
    "    transcripts, waveforms = batch\n",
    "    predicted_transcripts = speech_to_text(waveforms)\n",
    "\n",
    "    for predicted_transcript, transcript in zip(predicted_transcripts, transcripts):\n",
    "        m.append(\n",
    "            {\n",
    "                \"transcript\": transcript,\n",
    "                \"predicted_transcript\": predicted_transcript,\n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.DataFrame.from_dict(m)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
